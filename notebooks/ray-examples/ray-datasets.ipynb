{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Data Test Notebook\n",
    "\n",
    "The vast majority of this notebook is based off implementing the examples made available by the ray data [getting started](https://docs.ray.io/en/latest/data/getting-started.html#datasets-getting-started) docs. \n",
    "\n",
    "\n",
    "\n",
    "### What kinds of things should I use Ray Datasets for? \n",
    "\n",
    "According to their docs, Ray is, \"designed to load and pre-process data for distributed ML training pipelines...Ray Datasets is not intended as a replacement for more general data processing systems\"[[1]]. Its purpose is only to serve as a \"last mile\" distributed data processing tool. Therefore it is designed with the following 3 use cases in mind. \n",
    "\n",
    "* Last Mile Processing\n",
    "* Parallel Batch Inference\n",
    "* ML Training Ingest (Distributed training)\n",
    "\n",
    "Below we will attempt to evaluate Ray for these different types of use cases. \n",
    "\n",
    "_Note: current testing / evaluation done on a local PC with 32GB memory. This will need to be scaled down to work on ODH with current pod resource sizes i think._ \n",
    "\n",
    "[1]: https://docs.ray.io/en/master/data/faq.html#what-should-i-use-ray-datasets-for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data.aggregate import Mean, Std\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to our remote ray cluster if we're on an ODH notebook image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Ray Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are going to test the capabilities of this Ray data tool, we are going to need a reasonably sized example data set. Let's create a CSV file that's almost 1GB and save it to our current file system.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"tmp/output\") == False:\n",
    "    print(\"creating dataset\")\n",
    "    %memit \\\n",
    "    ds = ray.data.range(1000)\n",
    "    print(\"writing file\")\n",
    "    ds.repartition(1).write_csv(\"tmp/output\")\n",
    "    del ds\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"file exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our \"BIG\" dataset, let's read it in with Ray vs vanilla pandas, run some basic data transformations and compare each's memory foot print.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.util.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 19:20:43,627\tINFO packaging.py:388 -- Creating a file package for local directory 'tmp/output/'.\n",
      "2022-07-26 19:20:43,629\tINFO packaging.py:241 -- Pushing file package 'gcs://_ray_pkg_a8f1e79c71aa49ef.zip' (0.00MiB) to Ray cluster...\n",
      "2022-07-26 19:20:43,631\tINFO packaging.py:243 -- Successfully pushed file package 'gcs://_ray_pkg_a8f1e79c71aa49ef.zip'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClientContext(dashboard_url='10.128.3.105:8265', python_version='3.8.12', ray_version='1.12.1', ray_commit='4863e33856b54ccf8add5cbe75e41558850a1b75', protocol_version='2022-03-16', _num_clients=1, _context_to_restore=<ray.util.client._ClientContext object at 0x7fead5935bb0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init('ray://{ray_head}:10001'.format(ray_head=os.environ['RAY_CLUSTER']), runtime_env={\"working_dir\": \"tmp/output/\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'47873bc951f247189d8aa97e11d13ed7_000000.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = os.listdir(\"tmp/output/\")[0]\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 170.59 MiB, increment: 1.30 MiB\n",
      "CPU times: user 75.6 ms, sys: 23.3 ms, total: 98.9 ms\n",
      "Wall time: 219 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_line = pd.read_csv(f\"tmp/output/{file}\")\n",
    "ds_line.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 170.90 MiB, increment: 0.06 MiB\n",
      "CPU times: user 85.5 ms, sys: 21.5 ms, total: 107 ms\n",
      "Wall time: 225 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_line = pd.read_csv(f\"tmp/output/{file}\")\n",
    "ds_line.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 171.15 MiB, increment: 0.00 MiB\n",
      "CPU times: user 78.5 ms, sys: 48.6 ms, total: 127 ms\n",
      "Wall time: 257 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_line[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 171.40 MiB, increment: 0.21 MiB\n",
      "CPU times: user 80.3 ms, sys: 22.5 ms, total: 103 ms\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_line.applymap(lambda x: x *2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 172.02 MiB, increment: 0.58 MiB\n",
      "CPU times: user 86.4 ms, sys: 38.9 ms, total: 125 ms\n",
      "Wall time: 255 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    value\n",
       "6       6\n",
       "7       7\n",
       "8       8\n",
       "9       9\n",
       "10     10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit \\\n",
    "ds_line = ds_line[ds_line[\"value\"] > 5]\n",
    "ds_line.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(num_blocks=1, num_rows=None, schema={value: int64})\n"
     ]
    }
   ],
   "source": [
    "### Wokers don't have PVC access...So this won't work like locally \n",
    "#%%time\n",
    "ds_dst = ray.data.read_csv(file) #f\"tmp/output/{file}\")\n",
    "print(ds_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 180.79 MiB, increment: 0.57 MiB\n",
      "CPU times: user 77 ms, sys: 26.7 ms, total: 104 ms\n",
      "Wall time: 230 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 181.09 MiB, increment: 0.29 MiB\n",
      "CPU times: user 114 ms, sys: 25 ms, total: 139 ms\n",
      "Wall time: 1.95 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst.map_batches(lambda df:  df.applymap(lambda x: x *2), batch_format='pandas') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 181.12 MiB, increment: 0.02 MiB\n",
      "CPU times: user 103 ms, sys: 28.8 ms, total: 132 ms\n",
      "Wall time: 1.68 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'value': 6},\n",
       " {'value': 7},\n",
       " {'value': 8},\n",
       " {'value': 9},\n",
       " {'value': 10},\n",
       " {'value': 11},\n",
       " {'value': 12},\n",
       " {'value': 13},\n",
       " {'value': 14},\n",
       " {'value': 15}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst = ds_dst.map_batches(lambda df: df[df[\"value\"] > 5], batch_format=\"pandas\")\n",
    "ds_dst.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running all of the above cells looks to leave you with about a 20Gb memory load... May have to reset the kernel to move forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have a distributed dataset? not 1 file and partition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.util.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"tmp/output_dist\") == False:\n",
    "    print(\"creating dataset\")\n",
    "    %memit \\\n",
    "    ds = ray.data.range(1000)\n",
    "    print(\"writing file\")\n",
    "    ds.write_csv(\"tmp/output_dist\")\n",
    "    del ds\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"files exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.util.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 19:21:34,404\tINFO packaging.py:388 -- Creating a file package for local directory 'tmp/output_dist/'.\n",
      "2022-07-26 19:21:34,443\tINFO packaging.py:241 -- Pushing file package 'gcs://_ray_pkg_316e8530ea543222.zip' (0.04MiB) to Ray cluster...\n",
      "2022-07-26 19:21:34,446\tINFO packaging.py:243 -- Successfully pushed file package 'gcs://_ray_pkg_316e8530ea543222.zip'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClientContext(dashboard_url='10.128.3.105:8265', python_version='3.8.12', ray_version='1.12.1', ray_commit='4863e33856b54ccf8add5cbe75e41558850a1b75', protocol_version='2022-03-16', _num_clients=1, _context_to_restore=<ray.util.client._ClientContext object at 0x7fead5935bb0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init('ray://{ray_head}:10001'.format(ray_head=os.environ['RAY_CLUSTER']), runtime_env={\"working_dir\": \"tmp/output_dist/\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 182.55 MiB, increment: 0.87 MiB\n",
      "Dataset(num_blocks=200, num_rows=None, schema={value: int64})\n",
      "CPU times: user 110 ms, sys: 19 ms, total: 129 ms\n",
      "Wall time: 4.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst = ray.data.read_csv(f\"./\")\n",
    "print(ds_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 182.55 MiB, increment: 0.00 MiB\n",
      "CPU times: user 81.7 ms, sys: 16.4 ms, total: 98.1 ms\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 200/200 [00:05<00:00, 37.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 186.73 MiB, increment: 4.18 MiB\n",
      "CPU times: user 1.02 s, sys: 155 ms, total: 1.18 s\n",
      "Wall time: 6.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst.map_batches(lambda df:  df.applymap(lambda x: x *2), batch_format='pandas') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 200/200 [00:04<00:00, 41.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 186.81 MiB, increment: 0.02 MiB\n",
      "CPU times: user 1.14 s, sys: 217 ms, total: 1.36 s\n",
      "Wall time: 6.45 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'value': 6},\n",
       " {'value': 7},\n",
       " {'value': 8},\n",
       " {'value': 9},\n",
       " {'value': 10},\n",
       " {'value': 11},\n",
       " {'value': 12},\n",
       " {'value': 13},\n",
       " {'value': 14},\n",
       " {'value': 15}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst = ds_dst.map_batches(lambda df: df[df[\"value\"] > 5], batch_format=\"pandas\")\n",
    "ds_dst.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have evaluated 3 scenarios: Using vanilla pandas with a single dataset, using Ray with a single dataset and using Ray with a distributed dataset for a number of different operations. \n",
    "\n",
    "Our generated datasets are 100,000,000 rows long and 1 column wide, consisting only of integers. Below we have recorded the timing and memory results for loading the data, retrieving a slice (subset), applying the square function to each element and applying a filter to the dataset along with the total time taken to perform each step and the memory still in use after the entire set of operations ran. \n",
    "\n",
    "#### Ray vs Pandas performance results\n",
    "\n",
    "_These are the experimental results on an 8 core laptop with 32Gb Memory and should be repeated on an OPF cluster._ \n",
    "\n",
    "_TODO: Redo assessment once we get it working on the cluster._\n",
    "\n",
    "|                   |  Load      | Slice     |  Square     |  Filter   | Total Change | \n",
    "|-------------------|------------|-----------|-------------|-----------|--------------|\n",
    "|Pandas             | 10s, 800mb | 1s, 0mb   | 43s, 0mb    | 2s, 500mb | 56s, 1600mb  |\n",
    "|Ray (single block) | 9s, 1600mb | 5s,400mb  | 128s,2400mb | 86s,0mb   | 228s, 4400mb |\n",
    "|Ray (multi block)  | 1s, 1000mb | 5s, 400mb | 35s, 1100mb | 14s, 0mb  | 55s, 2500mb  |\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "From the table above we can see that using Ray data without dividing our dataset object into a reasonable number of blocks performs quite poorly. Its by far the slowest approach for the operations above and uses the most memory overall. \n",
    "For a smallish dataset like we are using here (~1Gb) vanilla pandas still works fairly well, however, it is still running as a single process and is not taking full advantage of the available resources. \n",
    "With the Ray Dataset divided into 200 blocks we get (in some cases) faster times than pandas with only about 1Gb more memory required. Furthermore, this approach maximizes use of the available resource on the machine. \n",
    "\n",
    "We are also able to convert Ray (single block) to Ray (multi block) and get the same increased performance by running a `ds.repartition(200)` command on our dataset. However, it is a somewhat expensive operation and should be avoided if possible.  \n",
    "\n",
    "\n",
    "_note: These are the experimental results on an 8 core laptop with 32Gb Memory and should be repeated on an OPF cluster._\n",
    "\n",
    "_note 2: The memory values recorded above from %memit did not seem to accurately capture the amount of memory used by the multiple Ray processes, so the chart reflects total usage from machine while running above code and not the %memit values._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: ML Preprocessing \n",
    "\n",
    "In this section we will mostly follow the [\"dataset ml preprocessing\"](https://docs.ray.io/en/latest/data/examples/big_data_ingestion.html) section of the Ray data docs to evaluate some of the \"last mile\" type of processing we'd want to use Ray for in a machine learning pipeline. Specifically we will perform the following 3 types of operations:\n",
    "\n",
    "1. Data Cleaning\n",
    "2. Aggregation and scaling\n",
    "3. Random Shuffle\n",
    "\n",
    "The first thing we need to do is create a slightly more complex data set, one that has 3 columns with proper column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.util.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files exists\n"
     ]
    }
   ],
   "source": [
    "# make a multi-column data set\n",
    "if os.path.exists(\"tmp/output_multi_col\") == False:\n",
    "    print(\"creating dataset\")\n",
    "    %memit \\\n",
    "    ds = ray.data.from_items([{\"A\":i%3,\"B\":i * 2,\"C\":i * 3} for i in range(200)])\n",
    "    print(\"writing file\")\n",
    "    ds.write_csv(\"tmp/output_multi_col\")\n",
    "    del ds\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"files exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.util.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 19:22:16,582\tINFO packaging.py:388 -- Creating a file package for local directory 'tmp/output_multi_col/'.\n",
      "2022-07-26 19:22:16,614\tINFO packaging.py:241 -- Pushing file package 'gcs://_ray_pkg_8027f6b348e358d6.zip' (0.04MiB) to Ray cluster...\n",
      "2022-07-26 19:22:16,616\tINFO packaging.py:243 -- Successfully pushed file package 'gcs://_ray_pkg_8027f6b348e358d6.zip'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClientContext(dashboard_url='10.128.3.105:8265', python_version='3.8.12', ray_version='1.12.1', ray_commit='4863e33856b54ccf8add5cbe75e41558850a1b75', protocol_version='2022-03-16', _num_clients=1, _context_to_restore=<ray.util.client._ClientContext object at 0x7fead5935bb0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init('ray://{ray_head}:10001'.format(ray_head=os.environ['RAY_CLUSTER']), runtime_env={\"working_dir\": \"tmp/output_multi_col/\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "Cool, let's encapsulate all the data cleaning steps we want to perform on our data into a single function. This is good practice in general, but will also let us pass this function to ray to be run in parallel on our dataset. \n",
    "\n",
    "All the moves below are arbitrary and selected just to show what's possible :)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(df: pd.DataFrame):\n",
    "    # Drop nulls.\n",
    "    df = df.dropna(subset=[\"A\"])\n",
    "    # Add new column.\n",
    "    df[\"new_col\"] = df[\"A\"] - 2 * df[\"B\"] + df[\"C\"] / 3\n",
    "    # Transform existing column.\n",
    "    df[\"A\"] = 2 * df[\"A\"] + 1\n",
    "    # Drop column.\n",
    "    df.drop(columns=\"B\", inplace=True)\n",
    "    # Re-add column \n",
    "    df[\"B\"] = df[\"C\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in our new dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 187.26 MiB, increment: 0.36 MiB\n",
      "Dataset(num_blocks=200, num_rows=None, schema={A: int64, B: int64, C: int64})\n",
      "CPU times: user 102 ms, sys: 26.1 ms, total: 128 ms\n",
      "Wall time: 4.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds = ray.data.read_csv(f\"./\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformations to our dataset in parallel on each block.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 200/200 [00:05<00:00, 36.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 s, sys: 142 ms, total: 1.21 s\n",
      "Wall time: 6.28 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'A': 1, 'C': 0, 'new_col': 0.0, 'B': 0},\n",
       " {'A': 3, 'C': 3, 'new_col': -2.0, 'B': 3},\n",
       " {'A': 5, 'C': 6, 'new_col': -4.0, 'B': 6},\n",
       " {'A': 1, 'C': 9, 'new_col': -9.0, 'B': 9},\n",
       " {'A': 3, 'C': 12, 'new_col': -11.0, 'B': 12}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ds = ds.map_batches(transform_batch, batch_format=\"pandas\")\n",
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND, for good measure, let's compare timing of loading our dataset and running our cleaning function using regular old pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 190.50 MiB, increment: 2.32 MiB\n",
      "CPU times: user 286 ms, sys: 15 ms, total: 301 ms\n",
      "Wall time: 410 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "files = os.listdir(\"tmp/output_multi_col\")\n",
    "files = [f\"tmp/output_multi_col/{file}\" for file in files]\n",
    "%memit ds_panda = pd.concat(map(pd.read_csv, files))\n",
    "ds_panda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.39 ms, sys: 1.08 ms, total: 6.46 ms\n",
      "Wall time: 5.49 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>new_col</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>531</td>\n",
       "      <td>-531.0</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>159</td>\n",
       "      <td>-157.0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>177</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>516</td>\n",
       "      <td>-515.0</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>-189.0</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A    C  new_col    B\n",
       "0  1  531   -531.0  531\n",
       "0  5  159   -157.0  159\n",
       "0  5  177   -175.0  177\n",
       "0  3  516   -515.0  516\n",
       "0  1  189   -189.0  189"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ds_panda = transform_batch(ds_panda)\n",
    "ds_panda.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations and Scaling\n",
    "\n",
    "Now let's looks at a few operations like getting the mean, std, and scaling our data set that require knowledge of the whole dataset making them a little more difficult to parallelize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupBy Map: 100%|██████████| 200/200 [00:02<00:00, 75.92it/s] \n",
      "GroupBy Reduce: 100%|██████████| 1/1 [00:00<00:00,  9.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "298.5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "ds.mean(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupBy Map: 100%|██████████| 200/200 [00:02<00:00, 77.09it/s] \n",
      "GroupBy Reduce: 100%|██████████| 1/1 [00:00<00:00,  8.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean(B)': 298.5, 'mean(C)': 298.5}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "ds.mean([\"B\", \"C\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we'll run the same operations with pandas so we have something to compare our results to.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "B    298.5\n",
       "C    298.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "ds_panda[[\"B\",\"C\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupBy Map: 100%|██████████| 200/200 [00:02<00:00, 78.85it/s] \n",
      "GroupBy Reduce: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 836 ms, sys: 74.3 ms, total: 910 ms\n",
      "Wall time: 3.34 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean(B)': 298.5, 'std(B)': 173.63755354185338, 'mean(C)': 298.5, 'std(C)': 173.63755354185338, 'mean(new_col)': -297.505, 'std(new_col)': 173.63656949597816}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "stats = ds.aggregate(Mean(\"B\"), Std(\"B\"), Mean(\"C\"), Std(\"C\"), Mean(\"new_col\"), Std(\"new_col\") )\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_standard_scaler(df: pd.DataFrame):\n",
    "    def column_standard_scaler(s: pd.Series):\n",
    "        s_mean = stats[f\"mean({s.name})\"]\n",
    "        s_std = stats[f\"std({s.name})\"]\n",
    "        return (s - s_mean) / s_std\n",
    "\n",
    "    cols = df.columns.difference([\"A\"])\n",
    "    df.loc[:, cols] = df.loc[:, cols].transform(column_standard_scaler)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 200/200 [00:03<00:00, 52.67it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'A': 1, 'C': -1.7190981669069068, 'new_col': 1.713377549807507, 'B': -1.7190981669069068},\n",
       " {'A': 3, 'C': -1.7018207983952796, 'new_col': 1.701859238856044, 'B': -1.7018207983952796},\n",
       " {'A': 5, 'C': -1.6845434298836524, 'new_col': 1.690340927904581, 'B': -1.6845434298836524},\n",
       " {'A': 1, 'C': -1.6672660613720252, 'new_col': 1.6615451505259233, 'B': -1.6672660613720252},\n",
       " {'A': 3, 'C': -1.6499886928603977, 'new_col': 1.6500268395744604, 'B': -1.6499886928603977}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "ds = ds.map_batches(batch_standard_scaler, batch_format=\"pandas\")\n",
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle\n",
    "\n",
    "When running ML training pipelines it is considered good practice to shuffle our training set at the beginning of each epoch. Let's look at a couple different ways we can shuffle our data with Ray.  \n",
    "\n",
    "* First we will shuffle the whole dataset once\n",
    "* Then we will shuffle it N times\n",
    "* Finally, we create a DatasetPipeline object that will shuffle each block when called in an iteration loop (like we would do for training) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|██████████| 200/200 [00:05<00:00, 35.01it/s]\n",
      "Shuffle Reduce:   0%|          | 0/200 [00:00<?, ?it/s]/opt/app-root/lib64/python3.8/site-packages/ray/util/client/worker.py:599: UserWarning: More than 10MB of messages have been created to schedule tasks on the server. This can be slow on Ray Client due to communication overhead over the network. If you're running many fine-grained tasks, consider running them inside a single remote function. See the section on \"Too fine-grained tasks\" in the Ray Design Patterns document for more details: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.f7ins22n6nyl. If your functions frequently use large objects, consider storing the objects remotely with ray.put. An example of this is shown in the \"Closure capture of large / unserializable object\" section of the Ray Design Patterns document, available here: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.1afmymq455wu\n",
      "  warnings.warn(\n",
      "Shuffle Reduce: 100%|██████████| 200/200 [00:09<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(num_blocks=200, num_rows=200, schema={A: int64, C: float64, new_col: float64, B: float64})\n",
      "CPU times: user 8.03 s, sys: 3.09 s, total: 11.1 s\n",
      "Wall time: 15.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'A': 5, 'C': 0.23324447490696723, 'new_col': -0.22745784551401657, 'B': 0.23324447490696723},\n",
       " {'A': 5, 'C': -1.062558163465073, 'new_col': 1.0683521365255764, 'B': -1.062558163465073},\n",
       " {'A': 3, 'C': -1.0280034264418185, 'new_col': 1.0280380481954556, 'B': -1.0280034264418185},\n",
       " {'A': 5, 'C': 1.2180544800697177, 'new_col': -1.2122734318641073, 'B': 1.2180544800697177},\n",
       " {'A': 5, 'C': -0.8552297413255465, 'new_col': 0.8610225393992416, 'B': -0.8552297413255465}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Shuffle once\n",
    "ds = ds.random_shuffle()\n",
    "print(ds)\n",
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|██████████| 200/200 [00:34<00:00,  5.82it/s]\n",
      "Shuffle Reduce: 100%|██████████| 200/200 [00:12<00:00, 16.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.84 s, sys: 4.24 s, total: 14.1 s\n",
      "Wall time: 46.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=20, num_stages=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Shuffle N times\n",
    "ds.random_shuffle().repeat(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 0:   5%|▌         | 1/20 [00:00<00:11,  1.61it/s]\n",
      "Stage 0:  10%|█         | 2/20 [01:02<10:57, 36.53s/it]\u001b[A\n",
      "Stage 0:  15%|█▌        | 3/20 [02:24<16:18, 57.57s/it]\u001b[A\n",
      "Stage 0:  20%|██        | 4/20 [03:45<17:47, 66.73s/it]\u001b[A\n",
      "Stage 0:  25%|██▌       | 5/20 [05:07<18:01, 72.13s/it]\u001b[A\n",
      "Stage 0:  30%|███       | 6/20 [06:27<17:30, 75.01s/it]\u001b[A\n",
      "Stage 0:  35%|███▌      | 7/20 [07:48<16:40, 76.94s/it]\u001b[A\n",
      "Stage 0:  40%|████      | 8/20 [09:10<15:39, 78.33s/it]\u001b[A\n",
      "Stage 0:  45%|████▌     | 9/20 [10:32<14:33, 79.44s/it]\u001b[A\n",
      "Stage 0:  50%|█████     | 10/20 [11:53<13:19, 79.92s/it][A\n",
      "Stage 0:  55%|█████▌    | 11/20 [13:15<12:05, 80.56s/it]\u001b[A\n",
      "Stage 0:  60%|██████    | 12/20 [14:36<10:45, 80.69s/it]\u001b[A\n",
      "Stage 0:  65%|██████▌   | 13/20 [15:57<09:26, 80.96s/it]\u001b[A\n",
      "Stage 0:  70%|███████   | 14/20 [17:18<08:05, 80.95s/it]\u001b[A\n",
      "Stage 0:  75%|███████▌  | 15/20 [18:40<06:46, 81.24s/it]\u001b[A\n",
      "Stage 0:  80%|████████  | 16/20 [20:01<05:24, 81.16s/it]\u001b[A\n",
      "Stage 0:  85%|████████▌ | 17/20 [21:21<04:02, 80.94s/it]\u001b[A\n",
      "Stage 0:  90%|█████████ | 18/20 [22:42<02:41, 80.77s/it]\u001b[A\n",
      "Stage 0:  95%|█████████▌| 19/20 [24:00<01:19, 79.97s/it]\u001b[A\n",
      "Stage 0: 100%|██████████| 20/20 [25:18<00:00, 79.35s/it]\u001b[A\n",
      "Stage 1: 100%|██████████| 20/20 [26:38<00:00, 79.91s/it]\u001b[A\n",
      "Stage 0: 100%|██████████| 20/20 [26:38<00:00, 79.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 51s, sys: 1min 38s, total: 5min 30s\n",
      "Wall time: 26min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# create a pipeline that trigger a random shuffle before each batch (epoch)\n",
    "ds = ds.repeat(num_epochs).random_shuffle_each_window()\n",
    "\n",
    "n = 0\n",
    "for i in ds.iter_batches():\n",
    "    n += len(i)\n",
    "n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so from the above, we can see how to use Ray to apply some common \"last mile\" data processing types of transformations to our dataset in a parallel fashion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data Pipelines \n",
    "\n",
    "In this section we will mostly follow the examples from [\"pipelining-compute\"](https://docs.ray.io/en/latest/data/pipelining-compute.html) and [\"advanced-pipelines\"](https://docs.ray.io/en/latest/data/advanced-pipelines.html) from the Ray docs to demonstrate how and when to use \"DatasetPipelines\". \n",
    "\n",
    "According to the docs, \"Unlike Datasets, which execute all transformations synchronously, DatasetPipelines implement pipelined execution. This allows for the overlapped execution of data input (e.g., reading files), computation (e.g. feature preprocessing), and output (e.g., distributed ML training).\"\n",
    "\n",
    "We saw DatasetPipelines a bit in the earlier section for shuffling our data. Here will look into constructing slightly more complex pipelines. \n",
    "\n",
    "First things first; Let's build a small dataset we can convert into a DatasetPipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(num_blocks=200, num_rows=100000, schema=<class 'int'>)\n"
     ]
    }
   ],
   "source": [
    "base = ray.data.range(100000)\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `.window()` to convert our Dataset into a DatasetPipeline with 10 blocks per window (20 windows for 200 blocks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 19:26:27,404\tINFO dataset.py:2643 -- Created DatasetPipeline with 20 windows: 0.04MiB min, 0.04MiB max, 0.04MiB mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPipeline(num_windows=20, num_stages=2)\n"
     ]
    }
   ],
   "source": [
    "pipe = base.window(blocks_per_window=10)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to define some functions we want to apply to our Data through the DatasetPipeline approach. We then use `pipe.map(func_N)` to add them to our pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(i):\n",
    "    return i+1\n",
    "\n",
    "def func2(i):\n",
    "    return i *2\n",
    "\n",
    "def func3(i):\n",
    "    return i%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPipeline(num_windows=20, num_stages=5)\n"
     ]
    }
   ],
   "source": [
    "pipe = pipe.map(func1)\n",
    "pipe = pipe.map(func2)\n",
    "pipe = pipe.map(func3)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the DatasetPipeline is defined, we have to iterate over it for it to trigger the computations we've defined on it. To do that let's just run a quick for loop over data batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 1:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Stage 0:  10%|█         | 2/20 [00:01<00:15,  1.16it/s]\u001b[A\n",
      "Stage 0:  15%|█▌        | 3/20 [00:01<00:09,  1.72it/s]\u001b[A\n",
      "Stage 0:  20%|██        | 4/20 [00:02<00:07,  2.15it/s]\u001b[A\n",
      "Stage 0:  25%|██▌       | 5/20 [00:02<00:05,  2.64it/s]\u001b[A\n",
      "Stage 0:  30%|███       | 6/20 [00:02<00:04,  3.09it/s]\u001b[A\n",
      "Stage 0:  35%|███▌      | 7/20 [00:02<00:03,  3.50it/s]\u001b[A\n",
      "Stage 0:  40%|████      | 8/20 [00:03<00:03,  3.56it/s]\u001b[A\n",
      "Stage 0:  45%|████▌     | 9/20 [00:03<00:02,  3.80it/s]\u001b[A\n",
      "Stage 0:  50%|█████     | 10/20 [00:03<00:02,  4.08it/s][A\n",
      "Stage 0:  55%|█████▌    | 11/20 [00:03<00:02,  3.99it/s]\u001b[A\n",
      "Stage 0:  60%|██████    | 12/20 [00:04<00:01,  4.10it/s]\u001b[A\n",
      "Stage 0:  65%|██████▌   | 13/20 [00:04<00:01,  4.29it/s]\u001b[A\n",
      "Stage 0:  70%|███████   | 14/20 [00:04<00:01,  4.47it/s]\u001b[A\n",
      "Stage 0:  75%|███████▌  | 15/20 [00:04<00:01,  4.19it/s]\u001b[A\n",
      "Stage 0:  80%|████████  | 16/20 [00:04<00:00,  4.35it/s]\u001b[A\n",
      "Stage 0:  85%|████████▌ | 17/20 [00:05<00:00,  4.48it/s]\u001b[A\n",
      "Stage 0:  90%|█████████ | 18/20 [00:05<00:00,  4.55it/s]\u001b[A\n",
      "Stage 0:  95%|█████████▌| 19/20 [00:05<00:00,  4.31it/s]\u001b[A\n",
      "Stage 0: 100%|██████████| 20/20 [00:05<00:00,  4.42it/s]\u001b[A\n",
      "Stage 1: 100%|██████████| 20/20 [00:06<00:00,  3.31it/s]\u001b[A\n",
      "Stage 0: 100%|██████████| 20/20 [00:06<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_rows = 0\n",
    "for row in pipe.iter_batches():\n",
    "    num_rows += len(row) \n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! now we know how to create, define and run DatasetPipelines with Ray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Large Scale ML Ingest Example\n",
    "\n",
    "Here we will go ahead an follow the [\"Big Data Ingestion\"](https://docs.ray.io/en/latest/data/examples/big_data_ingestion.html) example from the Ray docs. \n",
    "\n",
    "The goal here is to tie together everything above into a single demo that reflects a more _realistic_ scenario on how we would apply the Ray Data toolkit to a parallel and distributed machine learning use case.  \n",
    "\n",
    "First thing we will do is define a function called `create_shuffle_pipeline` that will turn our Dataset into a DatasetPipeline that will read in our data for each epoch, shuffle it and split it into equally sized shards for distributed training on multiple workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def create_shuffle_pipeline(training_data_dir: str, num_epochs: int, num_shards: int):\n",
    "\n",
    "    return (\n",
    "        ray.data.read_csv(training_data_dir)\n",
    "        .repeat(num_epochs)\n",
    "        .random_shuffle_each_window()\n",
    "        .split(num_shards, equal=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will define our own remote `TrainingWorker` class that iterates over our shards during training. For simplicity we will simple `pass` our training step as we are focused on just the distributed data processing steps here (keep things simple).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class TrainingWorker:\n",
    "    def __init__(self, rank, shard):\n",
    "        self.rank = rank\n",
    "        self.shard = shard\n",
    "\n",
    "    def train(self):\n",
    "        for epoch, training_dataset in enumerate(self.shard.iter_epochs()):\n",
    "            # Following code emulates epoch based SGD training.\n",
    "            print(f\"Training... worker: {self.rank}, epoch: {epoch}\")\n",
    "            for i, batch in enumerate(training_dataset.iter_batches()):\n",
    "                # TODO: replace the code for real training.\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the two key variables for this example, the number of Ray workers we'll use and the number of epochs to run. With the appropriate cluster resources, we can scale up our data ingest here by increasing the number of workers.\n",
    "\n",
    "According to the docs this whole process can be linearly scaled to arbitrarily large data sets (example is 500gb) by adding more nodes to our cluster and increasing our `NUM_TRAINING_WORKERS`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAINING_WORKERS = 2\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our DatasetPipeline called `splits` and instantiate our list of `TrainingWorkers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 ms, sys: 2.32 ms, total: 4.19 ms\n",
      "Wall time: 5.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "splits = create_shuffle_pipeline.remote(f\"./\", NUM_EPOCHS, NUM_TRAINING_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ray.get(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.35 ms, sys: 304 µs, total: 8.66 ms\n",
      "Wall time: 11.1 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|          | 0/3 [00:00<?, ?it/s]=6776)\u001b[0m \n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[Aor pid=6776)\u001b[0m \n",
      "Stage 1:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A76)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_workers = [TrainingWorker.options(name=f\"{rank}-{shard}\").remote(rank, shard) for rank, shard in enumerate(splits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use, `ray.get` to train our remote training_workers in parallel! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:  33%|███▎      | 1/3 [00:04<00:09,  4.74s/it]0m \n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=6776)\u001b[0m \n",
      "Stage 0:  67%|██████▋   | 2/3 [00:16<00:08,  8.83s/it]\u001b[A\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=6776)\u001b[0m \n",
      "Stage 0: 100%|██████████| 3/3 [00:27<00:00,  9.78s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=448, ip=10.128.3.109)\u001b[0m Training... worker: 0, epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]109)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=289, ip=10.131.2.37)\u001b[0m Training... worker: 1, epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]37)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=6776)\u001b[0m \n",
      "Stage 1: 100%|██████████| 3/3 [00:37<00:00, 11.91s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=289, ip=10.131.2.37)\u001b[0m Training... worker: 1, epoch: 1\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=448, ip=10.128.3.109)\u001b[0m Training... worker: 0, epoch: 1\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=448, ip=10.128.3.109)\u001b[0m Training... worker: 0, epoch: 2\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=289, ip=10.131.2.37)\u001b[0m Training... worker: 1, epoch: 2\n",
      "CPU times: user 97.6 ms, sys: 28.2 ms, total: 126 ms\n",
      "Wall time: 33.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:10<00:00, 10.01s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]37)\u001b[0m \n",
      "Stage 0: 100%|██████████| 1/1 [00:10<00:00, 10.60s/it] \n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 13.37it/s] \n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]109)\u001b[0m \n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]37)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 12.67it/s] \n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ray.get([worker.train.remote() for worker in training_workers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.util.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Congrats!\n",
    "\n",
    "If you are looking at this cell and there are no error above, you know that Ray Data is working! "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e34450d332bd313b8f818cb5ed04e25933b13de9c0d7b662ddcaf48d79a536f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
