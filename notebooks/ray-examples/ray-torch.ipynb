{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray with Pytorch Test Notebook\n",
    "\n",
    "The purpose of this notebook is to confirm that Ray Train with pytorch works in ODH.\n",
    "\n",
    "This notebook primarily consists of an implementation of the pytorch example from the Ray docs on [Ray Train](https://docs.ray.io/en/latest/train/train.html). \n",
    "\n",
    "However, it has been modified to test that the Ray Train features for pytorch work in an Open Data Hub environment. We have also increased the number of samples and epochs run so that the speed up from Ray's distribution can be seen clearly.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.utils import consume_prefix_in_state_dict_if_present\n",
    "\n",
    "from ray import train\n",
    "import ray.train.torch\n",
    "from ray.train import Trainer\n",
    "from ray.train import CheckpointStrategy\n",
    "\n",
    "\n",
    "from ray.util import connect as ray_connect\n",
    "from ray.util import disconnect as ray_disconnect\n",
    "from ray.util.client import ray as rayclient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We're going to connect to our Ray Cluster that was spun up for us as part of the [ray notebook image](https://github.com/thoth-station/ray-ml-notebook) we selected through the ODH spawner page. \n",
    "\n",
    "This cell should also run locally without a Ray cluster as it checks for the relevant environment variable \"RAY_CLUSTER\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClientContext(dashboard_url='10.131.2.216:8265', python_version='3.8.12', ray_version='1.12.1', ray_commit='4863e33856b54ccf8add5cbe75e41558850a1b75', protocol_version='2022-03-16', _num_clients=2, _context_to_restore=<ray.util.client._ClientContext object at 0x7f82c1f5bf70>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init('ray://{ray_head}:10001'.format(ray_head=os.environ['RAY_CLUSTER']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we are interested in the speed of training, not the accuracy. So, let's make our lives easy and just generate some random dataset. We are going to construct a simple feed forward neural network with an feature size of 10 and an output size of 5. We will construct our dummy dataset accordingly. \n",
    "\n",
    "We also want to show the benefits of Ray so we will create a somewhat large dataset with 200,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "\n",
    "input = torch.randn(num_samples, input_size)\n",
    "labels = torch.randn(num_samples, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our simple pytorch Neural Network with a simple forward pass function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = 15\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        y_pred = self.layer1(input)\n",
    "        y_pred = self.relu(y_pred)\n",
    "        y_pred = self.layer2(y_pred)\n",
    "        \n",
    "        return y_pred \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Train\n",
    "Now we will define our non-Ray training function for a baseline we will use for timing comparisons later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func():\n",
    "    num_epochs = 31\n",
    "    model = NeuralNetwork()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"epoch: {epoch}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's run our training function and see how long it takes without using any of the Ray Train's distribution functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.2456941604614258\n",
      "CPU times: user 12.5 ms, sys: 897 µs, total: 13.4 ms\n",
      "Wall time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Train\n",
    "Above we can see how long it took our non-distributed training function to iterate through all its epochs. \n",
    "\n",
    "Below we define our Ray Train distributed training function, which in this case requires only that we add the line: \n",
    "```\n",
    "model = train.torch.prepare_model(model)\n",
    "```\n",
    "\n",
    "_Note:_ The initial training function was written knowing it would later be distributed with Ray. We make no claim that converting existing pytorch code to Ray Train compatible pytorch is always as simple as adding a single line of code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_distributed():\n",
    "    \n",
    "    dev = \"cuda\"\n",
    "    #device = torch.device(dev)\n",
    "    \n",
    "    num_epochs = 31\n",
    "    model = NeuralNetwork().to(dev)\n",
    "    model = train.torch.prepare_model(model)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        output = model(input.to(dev))\n",
    "        loss = loss_fn(output.to(dev), labels.to(dev)) # both of these might need \"to_device\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"epoch: {epoch}, loss: {loss.item()}\")\n",
    "            state_dict = model.state_dict()\n",
    "            consume_prefix_in_state_dict_if_present(state_dict, \"module.\")\n",
    "            train.save_checkpoint(epoch=epoch, model_weights=state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate our Ray `Trainer` that we use to manage which backend we want (pytorch, tensorflow or horovod) and the number of workers we will want to use. Below we will use 4 workers if we are connected to a Ray cluster, otherwise we will just use 1. Here we can also define whether or not we want to use a gpu for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 16:00:43,006\tINFO trainer.py:223 -- Trainer logs will be logged in: /opt/app-root/src/ray_results/train_2022-07-21_16-00-43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.99 ms, sys: 622 µs, total: 10.6 ms\n",
      "Wall time: 609 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.environ.get('RAY_CLUSTER') is not None:\n",
    "    num_workers = 2\n",
    "else:\n",
    "    num_workers = 1\n",
    "\n",
    "trainer = Trainer(backend=\"torch\", num_workers=num_workers, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Let's run our training function and see how long it takes with Ray Train's distribution functionality. Please note, there is an overhead cost associated with starting the Trainer that seems to take around ~4 seconds. So let's time that separately from our actual training function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 4.4 ms, total: 18.8 ms\n",
      "Wall time: 4.87 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=304, ip=10.131.0.28)\u001b[0m 2022-07-21 16:00:48,292\tINFO torch.py:334 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=1264)\u001b[0m 2022-07-21 16:00:48,319\tINFO torch.py:334 -- Setting up process group for: env:// [rank=1, world_size=2]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 16:00:48,492\tINFO trainer.py:229 -- Run results will be logged in: /opt/app-root/src/ray_results/train_2022-07-21_16-00-43/run_001\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=304, ip=10.131.0.28)\u001b[0m 2022-07-21 16:00:51,723\tINFO torch.py:92 -- Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=304, ip=10.131.0.28)\u001b[0m 2022-07-21 16:00:51,723\tINFO torch.py:126 -- Wrapping provided model in DDP.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=1264)\u001b[0m 2022-07-21 16:00:51,931\tINFO torch.py:92 -- Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=1264)\u001b[0m 2022-07-21 16:00:51,932\tINFO torch.py:126 -- Wrapping provided model in DDP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=1264)\u001b[0m epoch: 0, loss: 1.201425552368164\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=304, ip=10.131.0.28)\u001b[0m epoch: 0, loss: 1.201425552368164\n",
      "CPU times: user 26.4 ms, sys: 6.56 ms, total: 33 ms\n",
      "Wall time: 3.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#checkpoint_strategy = CheckpointStrategy(num_to_keep=1)\n",
    "trainer.run(train_func_distributed)\n",
    "results = trainer.latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.63 ms, sys: 3.56 ms, total: 5.2 ms\n",
      "Wall time: 886 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Times will vary depending on where you are running this notebook, the sample size you selected above, the number of epochs and the number of workers, but if everything worked correctly and you are using a distributed ray cluster, the Wall time for the 'train.run()` function above should be significantly less than that for the non-distributed training run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "We've trained a model! Now we need to make sure we can use it for inference. Below we'll perform to quick examples of using the trained model. First, we'll generate a brand new data set the same way we did above, and use MSE to evaluate the model's performance. Second we will perform inference on a single new input and print out the result.  \n",
    "\n",
    "\n",
    "In both cases, since the data is random for this example we don't really care what the MSE or inference values are, we are simply illustrating that we can perform inference with our newly trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for predictions on new data: 1.395233154296875\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(num_samples, input_size)\n",
    "labels = torch.randn(num_samples, output_size)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(results[\"model_weights\"])\n",
    "model.eval()\n",
    "y_pred = model(input)\n",
    "\n",
    "MSE = nn.MSELoss()\n",
    "mse = MSE(labels,y_pred)\n",
    "print(f'MSE for predictions on new data: {mse.data.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37642398,  0.13873528, -0.1956471 ,  0.02701393, -0.18512742]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = torch.randn(1, input_size)\n",
    "model(input_sample).data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are reading this cell and there are no errors above, then you have successfully run a Ray Train Pytorch notebook in a distributed environment with Open Data Hub!  Yeahh!! :) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e34450d332bd313b8f818cb5ed04e25933b13de9c0d7b662ddcaf48d79a536f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
